# GPU í• ë‹¹ ê³„íš

## ğŸ“‹ ê°œìš”
ì‹¬í… PCB Inspection AI í”Œë«í¼ì˜ GPU ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì „ëµ

**ì—…ë°ì´íŠ¸**: 2025-01-11

---

## ğŸ¯ GPU ìš”êµ¬ì‚¬í•­

### GPU í•„ìš” ì„œë¹„ìŠ¤
```yaml
AI Inference (8001):
  GPU: í•„ìˆ˜
  ìš©ë„: ì‹¤ì‹œê°„ PCB ë¶ˆëŸ‰ ê²€ì¶œ
  ìš°ì„ ìˆœìœ„: ìµœê³  (P1)
  íŠ¹ì§•: 24/7 ì‹¤í–‰, ë‚®ì€ ì§€ì—°ì‹œê°„ í•„ìš”

AI Training (8002):
  GPU: í•„ìˆ˜
  ìš©ë„: YOLO ëª¨ë¸ í•™ìŠµ/ì¬í•™ìŠµ
  ìš°ì„ ìˆœìœ„: ë†’ìŒ (P2)
  íŠ¹ì§•: ë°°ì¹˜ ì‘ì—…, ìŠ¤ì¼€ì¤„ë§ ê°€ëŠ¥

Image Synthesis (8003):
  GPU: ê¶Œì¥
  ìš©ë„: GAN ê¸°ë°˜ ë¶ˆëŸ‰ ì´ë¯¸ì§€ ìƒì„±
  ìš°ì„ ìˆœìœ„: ì¤‘ê°„ (P3)
  íŠ¹ì§•: ì˜¨ë””ë§¨ë“œ ì‹¤í–‰
```

### GPU ë¶ˆí•„ìš” ì„œë¹„ìŠ¤
```yaml
- Frontend (Next.js)
- Backend Core
- Analytics
- Image Search (CPU + Qdrant)
- Monitoring
```

---

## ğŸ–¥ï¸ GPU í™˜ê²½ë³„ í• ë‹¹ ì „ëµ

### ì‹œë‚˜ë¦¬ì˜¤ 1: GPU 2ê°œ í™˜ê²½ (ê¶Œì¥)

```yaml
GPU 0 (ì¶”ë¡  ì „ìš©):
  Primary:
    - AI Inference (8001)
      - ë©”ëª¨ë¦¬: 4-6GB
      - ìƒì‹œ ì‹¤í–‰
      - ìµœê³  ìš°ì„ ìˆœìœ„

  Shared (ì—¬ìœ  ì‹œ):
    - Image Synthesis (8003)
      - ë©”ëª¨ë¦¬: 2-4GB
      - ì˜¨ë””ë§¨ë“œ ì‹¤í–‰
      - Inference ìš°ì„ 

GPU 1 (í•™ìŠµ ì „ìš©):
  Primary:
    - AI Training (8002)
      - ë©”ëª¨ë¦¬: 8-12GB
      - ë°°ì¹˜ ì‘ì—…
      - ìŠ¤ì¼€ì¤„ë§ ê°€ëŠ¥
```

**Docker Compose ì„¤ì •:**
```yaml
services:
  ai-inference:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  ai-training:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  image-synthesis:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 2: GPU 1ê°œ í™˜ê²½

```yaml
GPU 0 (ê³µìœ ):
  ìš°ì„ ìˆœìœ„ ìŠ¤ì¼€ì¤„ë§:
    1. AI Inference (8001) - í•­ìƒ ìš°ì„ 
       ë©”ëª¨ë¦¬: 4-6GB ì˜ˆì•½

    2. AI Training (8002) - Inference ë¯¸ì‚¬ìš© ì‹œ
       ë©”ëª¨ë¦¬: ë‚˜ë¨¸ì§€ ì „ë¶€
       ìŠ¤ì¼€ì¤„: ì•¼ê°„/ì£¼ë§

    3. Image Synthesis (8003) - ë‚®ì€ ìš°ì„ ìˆœìœ„
       ë©”ëª¨ë¦¬: 2-4GB
       ì‹¤í–‰ ì œí•œ: Training ì—†ì„ ë•Œë§Œ
```

**Docker Compose ì„¤ì •:**
```yaml
services:
  ai-inference:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
        limits:
          memory: 6G

  ai-training:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    # ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰ ì œì–´

  image-synthesis:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
```

**ì‹¤í–‰ ìˆœì„œ:**
```bash
# 1. InferenceëŠ” í•­ìƒ ì‹¤í–‰
docker-compose up -d ai-inference

# 2. Trainingì€ í•„ìš” ì‹œì—ë§Œ
docker-compose up -d ai-training
# ì™„ë£Œ í›„
docker-compose stop ai-training

# 3. SynthesisëŠ” ë…ë¦½ ì‹¤í–‰
docker-compose up -d image-synthesis
docker-compose stop image-synthesis
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 3: GPU ì—†ìŒ (ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½)

```yaml
AI Inference:
  - CPU ëª¨ë“œë¡œ ì‹¤í–‰
  - ONNX Runtime (CPU ìµœì í™”)
  - ì„±ëŠ¥: GPU ëŒ€ë¹„ 5-10ë°° ëŠë¦¼
  - ìš©ë„: ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ë§Œ

AI Training:
  - ì‹¤í–‰ ë¶ˆê°€ ë˜ëŠ” ë§¤ìš° ëŠë¦¼
  - ëŒ€ì•ˆ: í´ë¼ìš°ë“œ GPU (AWS/GCP)

Image Synthesis:
  - CPU ëª¨ë“œ ê°€ëŠ¥í•˜ë‚˜ ë§¤ìš° ëŠë¦¼
  - ëŒ€ì•ˆ: ì‚¬ì „ ìƒì„±ëœ ì´ë¯¸ì§€ ì‚¬ìš©
```

---

## ğŸ”§ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬

### ë©”ëª¨ë¦¬ í• ë‹¹ ê°€ì´ë“œ

```yaml
AI Inference:
  ëª¨ë¸ í¬ê¸°: YOLOv8 (2-3GB)
  ë°°ì¹˜ í¬ê¸°: 1-4
  í•„ìš” ë©”ëª¨ë¦¬: 4-6GB
  ê¶Œì¥: 8GB

AI Training:
  ëª¨ë¸ í¬ê¸°: YOLOv8 (2-3GB)
  ë°°ì¹˜ í¬ê¸°: 16-32
  ë°ì´í„° ë¡œë”©: 2-4GB
  í•„ìš” ë©”ëª¨ë¦¬: 8-12GB
  ê¶Œì¥: 16GB

Image Synthesis:
  GAN ëª¨ë¸: 1-2GB
  ë°°ì¹˜ í¬ê¸°: 4-8
  í•„ìš” ë©”ëª¨ë¦¬: 2-4GB
  ê¶Œì¥: 6GB
```

### PyTorch ë©”ëª¨ë¦¬ ìµœì í™”

```python
# services/ai-inference/app/core/model_manager.py

import torch

class ModelManager:
    def __init__(self):
        # GPU ë©”ëª¨ë¦¬ ì œí•œ
        torch.cuda.set_per_process_memory_fraction(0.6, device=0)

        # ë©”ëª¨ë¦¬ ìºì‹œ ë¹„ìš°ê¸°
        torch.cuda.empty_cache()

        # Mixed Precision (ë©”ëª¨ë¦¬ ì ˆì•½)
        self.use_amp = True

    def load_model(self):
        model = YOLOv8()
        model = model.cuda()
        model.eval()

        # Gradient ê³„ì‚° ë¹„í™œì„±í™” (ì¶”ë¡  ì‹œ)
        torch.set_grad_enabled(False)

        return model
```

---

## ğŸ“Š GPU ëª¨ë‹ˆí„°ë§

### nvidia-smi ëª…ë ¹ì–´

```bash
# GPU ì‚¬ìš©ë¥  í™•ì¸
nvidia-smi

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (1ì´ˆë§ˆë‹¤)
watch -n 1 nvidia-smi

# íŠ¹ì • GPUë§Œ
nvidia-smi -i 0

# ë©”ëª¨ë¦¬ë§Œ í™•ì¸
nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

### Docker ì»¨í…Œì´ë„ˆë³„ GPU ì‚¬ìš©ëŸ‰

```bash
# ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆì˜ GPU ì‚¬ìš©
docker stats pcb-ai-inference pcb-ai-training

# GPU ë©”ëª¨ë¦¬ í™•ì¸
nvidia-smi --query-compute-apps=pid,used_memory --format=csv
```

### Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```yaml
# infrastructure/observability/prometheus/prometheus.yml

scrape_configs:
  - job_name: 'nvidia_gpu'
    static_configs:
      - targets: ['node-exporter:9100']
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'nvidia_.*'
        action: keep
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. OOM (Out of Memory) ë°©ì§€
```python
# ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
def get_optimal_batch_size():
    gpu_memory = torch.cuda.get_device_properties(0).total_memory

    if gpu_memory < 8 * 1024**3:  # 8GB ë¯¸ë§Œ
        return 1
    elif gpu_memory < 16 * 1024**3:  # 16GB ë¯¸ë§Œ
        return 4
    else:
        return 8
```

### 2. ë©€í‹° í”„ë¡œì„¸ìŠ¤ ì£¼ì˜
```yaml
ë¬¸ì œ:
  - ê°™ì€ GPUì— ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ ë™ì‹œ ì‹¤í–‰
  - ë©”ëª¨ë¦¬ ë¶€ì¡± ë°œìƒ ê°€ëŠ¥

í•´ê²°:
  - Docker ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
  - í”„ë¡œì„¸ìŠ¤ ìŠ¤ì¼€ì¤„ë§
  - ìš°ì„ ìˆœìœ„ ê´€ë¦¬
```

### 3. GPU ì˜¨ë„ ê´€ë¦¬
```bash
# GPU ì˜¨ë„ í™•ì¸
nvidia-smi --query-gpu=temperature.gpu --format=csv

# 80ë„ ì´ìƒ ì‹œ ì•ŒëŒ (Prometheus)
```

---

## ğŸš€ ìµœì í™” íŒ

### 1. ëª¨ë¸ ìµœì í™”
```yaml
ONNX Runtime:
  - PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
  - ì¶”ë¡  ì†ë„ 2-3ë°° í–¥ìƒ
  - ë©”ëª¨ë¦¬ ì‚¬ìš© 30% ê°ì†Œ

TensorRT:
  - NVIDIA ì „ìš© ìµœì í™”
  - ì¶”ë¡  ì†ë„ 5-10ë°° í–¥ìƒ
  - ë³µì¡í•œ ë³€í™˜ ê³¼ì •
```

### 2. ë°°ì¹˜ ì²˜ë¦¬
```python
# ì‘ì€ ë°°ì¹˜ ì—¬ëŸ¬ë²ˆ ëŒ€ì‹  í° ë°°ì¹˜ í•œë²ˆ
# Bad
for image in images:
    result = model(image)

# Good
results = model(images_batch)
```

### 3. Mixed Precision
```python
# FP16 ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ 50% ì ˆì•½
from torch.cuda.amp import autocast

with autocast():
    output = model(input)
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì°¸ê³ )

### GPUë³„ ì¶”ë¡  ì„±ëŠ¥ (YOLOv8)

| GPU ëª¨ë¸        | ë°°ì¹˜ 1 | ë°°ì¹˜ 4 | ë°°ì¹˜ 8 | ë©”ëª¨ë¦¬  |
|----------------|--------|--------|--------|---------|
| RTX 3060 (12GB)| 45 FPS | 120 FPS| 150 FPS| 4-6 GB  |
| RTX 3070 (8GB) | 55 FPS | 140 FPS| 180 FPS| 4-6 GB  |
| RTX 3080 (10GB)| 70 FPS | 180 FPS| 240 FPS| 4-6 GB  |
| RTX 4090 (24GB)| 120 FPS| 300 FPS| 450 FPS| 4-6 GB  |

### í•™ìŠµ ì‹œê°„ (1 epoch, 1000 images)

| GPU ëª¨ë¸        | ì‹œê°„    | ë°°ì¹˜ í¬ê¸° | ë©”ëª¨ë¦¬   |
|----------------|---------|----------|---------|
| RTX 3060       | 45ë¶„    | 16       | 10 GB   |
| RTX 3070       | 35ë¶„    | 16       | 10 GB   |
| RTX 3080       | 25ë¶„    | 32       | 12 GB   |
| RTX 4090       | 12ë¶„    | 64       | 16 GB   |

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°°í¬ ì „ í™•ì¸ì‚¬í•­

```yaml
â˜ NVIDIA Driver ì„¤ì¹˜ í™•ì¸
  nvidia-smi ì‹¤í–‰ ê°€ëŠ¥

â˜ Docker GPU ì§€ì› í™•ì¸
  docker run --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

â˜ GPU ê°œìˆ˜ í™•ì¸
  nvidia-smi -L

â˜ ë©”ëª¨ë¦¬ ì¶©ë¶„í•œì§€ í™•ì¸
  ê° ì„œë¹„ìŠ¤ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ vs ì‹¤ì œ GPU ë©”ëª¨ë¦¬

â˜ docker-compose.yml GPU ì„¤ì • í™•ì¸
  device_ids ì˜¬ë°”ë¥¸ì§€

â˜ ëª¨ë‹ˆí„°ë§ ì„¤ì •
  Prometheus + Grafana GPU ë©”íŠ¸ë¦­
```

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-01-11
**ê¶Œì¥ GPU**: NVIDIA RTX 3060 ì´ìƒ (12GB VRAM)
